{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyZowh4tHINJJN4EarcYFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oluwafemidiakhoa/MLprject/blob/main/DNASEQUENCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61QPFicx8Q9w",
        "outputId": "43074c17-c541-4806-d440-f272535ea85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets tokenizers sentencepiece\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Define paths for the ZIP files\n",
        "zip_files = [\n",
        "    '/content/dog.txt.zip',\n",
        "    '/content/chimpanzee.txt (1).zip',\n",
        "    '/content/human.txt.zip'\n",
        "]\n",
        "\n",
        "# Directory to extract files\n",
        "extracted_dir = '/mnt/data/unzipped/'\n",
        "\n",
        "# Unzipping the files\n",
        "for zip_file in zip_files:\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extracted_dir)\n",
        "\n",
        "print(\"Files unzipped successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzNV4pnj8mfc",
        "outputId": "a286133f-86ed-4a8a-e3ca-eddc5a40c717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for the text files and FASTA file\n",
        "file_paths = [\n",
        "    os.path.join(extracted_dir, 'dog.txt'),\n",
        "    os.path.join(extracted_dir, 'chimpanzee.txt'),\n",
        "    os.path.join(extracted_dir, 'human.txt'),\n",
        "    '/content/example_dna.fa'\n",
        "]\n",
        "\n",
        "# Initialize a list to store sequences\n",
        "sequences = []\n",
        "\n",
        "# Read the text files\n",
        "for file_path in file_paths[:3]:  # The first three are text files\n",
        "    with open(file_path, 'r') as file:\n",
        "        sequences.extend(file.readlines())\n",
        "\n",
        "# Read the FASTA file\n",
        "with open(file_paths[3], 'r') as file:\n",
        "    sequences.extend([line.strip() for line in file if not line.startswith('>')])\n",
        "\n",
        "print(f\"Loaded {len(sequences)} sequences.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAhVfD8U9RzL",
        "outputId": "e31789c7-6b3d-4495-d25f-3b06db1fa636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 6928 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "class AdvancedBocaTokenizer:\n",
        "    def __init__(self, vocab_size=500):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.sp = spm.SentencePieceProcessor()\n",
        "\n",
        "    def train(self, sequences, model_prefix=\"boca_model\"):\n",
        "        # Write sequences to a temporary file for training\n",
        "        temp_file = \"sequences.txt\"\n",
        "        with open(temp_file, \"w\") as f:\n",
        "            for seq in sequences:\n",
        "                f.write(f\"{seq}\\n\")\n",
        "\n",
        "        # Define special tokens\n",
        "        special_tokens = [\"[MASK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"]\n",
        "\n",
        "        # Train the SentencePiece model\n",
        "        spm.SentencePieceTrainer.train(\n",
        "            input=temp_file,\n",
        "            model_prefix=model_prefix,\n",
        "            vocab_size=self.vocab_size,\n",
        "            character_coverage=1.0,  # Ensures full coverage of characters in DNA sequences\n",
        "            pad_id=3,  # Assuming [PAD] is the fourth token\n",
        "            unk_id=4,  # Assuming [UNK] is the fifth token\n",
        "            user_defined_symbols=special_tokens  # Adding special tokens\n",
        "        )\n",
        "\n",
        "        # Load the trained model\n",
        "        self.sp.load(f\"{model_prefix}.model\")\n",
        "\n",
        "        # Clean up\n",
        "        os.remove(temp_file)\n",
        "        print(\"Tokenizer trained successfully.\")\n",
        "\n",
        "    def tokenize(self, sequence):\n",
        "        return self.sp.encode(sequence, out_type=str)\n",
        "\n",
        "    def detokenize(self, tokens):\n",
        "        return self.sp.decode(tokens)\n",
        "\n",
        "    def save_vocab(self, path):\n",
        "        print(f\"Vocabulary saved at {path}.model\")\n",
        "\n",
        "    def load_vocab(self, path):\n",
        "        self.sp.load(f\"{path}.model\")\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AdvancedBocaTokenizer(vocab_size=500)\n",
        "\n",
        "# Train the tokenizer with the sequences\n",
        "tokenizer.train(sequences, model_prefix=\"dna_boca_tokenizer\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrBVVSD69nso",
        "outputId": "9c87cfc1-8786-48fd-b54c-a785988e8c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer trained successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize sequences using the trained SentencePiece model\n",
        "# The output should be a list of integers (token IDs)\n",
        "tokenized_sequences = [{\"input_ids\": tokenizer.sp.encode(seq, out_type=int)} for seq in sequences]\n",
        "print(\"Sequences tokenized successfully.\")\n",
        "\n",
        "# Example of tokenized output (now it should be lists of integers)\n",
        "print(tokenized_sequences[:5])  # Display the first 5 tokenized sequences\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZmS55qJh3S",
        "outputId": "9302154f-e67b-4a32-b5db-4e41da5656de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences tokenized successfully.\n",
            "[{'input_ids': [63, 493, 492, 498, 499, 492, 497, 494, 492, 63, 494, 495, 496, 493, 493]}, {'input_ids': [163, 73, 107, 149, 50, 73, 58, 29, 174, 187, 143, 321, 316, 106, 54, 230, 51, 174, 289, 389, 24, 321, 164, 84, 31, 41, 403, 200, 187, 19, 148, 171, 371, 75, 81, 65, 356, 78, 155, 65, 336, 16, 307, 45, 282, 278, 37, 314, 41, 232, 187, 63, 485]}, {'input_ids': [163, 11, 16, 307, 45, 282, 278, 37, 314, 41, 232, 187, 136, 206, 54, 226, 22, 436, 327, 151, 282, 212, 141, 93, 335, 446, 67, 170, 356, 480, 327, 27, 356, 203, 220, 309, 32, 20, 321, 417, 323, 65, 20, 146, 315, 438, 295, 264, 15, 241, 161, 107, 106, 146, 313, 21, 32, 117, 160, 93, 22, 425, 417, 245, 400, 16, 174, 367, 32, 426, 16, 92, 390, 390, 373, 187, 367, 33, 24, 315, 72, 54, 24, 402, 33, 67, 117, 140, 82, 209, 160, 35, 16, 192, 249, 119, 461, 103, 410, 184, 312, 19, 265, 54, 117, 116, 146, 114, 9, 216, 129, 160, 31, 27, 160, 93, 30, 346, 22, 69, 124, 202, 319, 295, 40, 409, 436, 178, 341, 357, 269, 391, 226, 360, 345, 79, 13, 69, 256, 409, 31, 246, 453, 52, 37, 256, 135, 93, 174, 417, 89, 135, 241, 89, 13, 117, 75, 478, 308, 321, 105, 333, 37, 86, 146, 334, 27, 160, 32, 90, 14, 96, 341, 20, 63, 485]}, {'input_ids': [406, 35, 42, 78, 84, 207, 24, 231, 283, 342, 158, 10, 361, 362, 136, 67, 361, 257, 31, 278, 23, 451, 118, 142, 344, 260, 118, 246, 302, 16, 361, 57, 22, 189, 28, 201, 138, 111, 283, 314, 13, 172, 23, 402, 347, 361, 39, 274, 128, 279, 428, 302, 92, 115, 10, 312, 57, 207, 211, 85, 153, 40, 302, 16, 287, 32, 233, 170, 26, 442, 133, 106, 107, 170, 142, 200, 107, 433, 105, 20, 30, 87, 481, 71, 100, 460, 73, 165, 60, 121, 152, 388, 410, 184, 347, 15, 120, 102, 219, 183, 36, 236, 278, 27, 250, 54, 263, 32, 36, 273, 159, 120, 304, 16, 62, 342, 338, 459, 342, 170, 108, 16, 76, 62, 16, 133, 118, 428, 40, 318, 83, 16, 215, 120, 254, 73, 92, 231, 92, 206, 60, 83, 86, 402, 84, 166, 361, 16, 330, 202, 38, 38, 75, 83, 71, 142, 78, 428, 33, 298, 36, 212, 277, 9, 272, 40, 97, 246, 100, 279, 302, 172, 128, 9, 425, 470, 165, 46, 71, 365, 57, 48, 184, 145, 15, 465, 92, 165, 125, 170, 393, 50, 320, 26, 178, 246, 484, 207, 126, 139, 342, 36, 55, 140, 393, 70, 69, 20, 415, 139, 166, 274, 427, 181, 338, 26, 76, 102, 378, 201, 26, 425, 126, 159, 42, 441, 23, 437, 52, 126, 171, 246, 402, 347, 180, 238, 159, 15, 420, 329, 154, 296, 342, 276, 432, 450, 175, 15, 34, 128, 228, 28, 63, 306]}, {'input_ids': [163, 36, 32, 35, 224, 56, 54, 245, 73, 57, 423, 24, 16, 180, 15, 118, 24, 16, 279, 300, 118, 325, 253, 227, 83, 227, 20, 233, 129, 97, 191, 26, 106, 161, 40, 457, 410, 262, 425, 124, 255, 207, 330, 255, 453, 360, 52, 257, 293, 84, 37, 51, 15, 168, 202, 33, 213, 246, 81, 39, 274, 478, 216, 441, 126, 298, 327, 21, 332, 189, 60, 397, 164, 26, 241, 363, 60, 144, 444, 67, 135, 179, 302, 118, 108, 18, 342, 108, 14, 118, 62, 165, 15, 207, 80, 42, 236, 121, 108, 278, 9, 46, 153, 385, 192, 10, 100, 241, 132, 273, 324, 73, 122, 170, 211, 16, 80, 384, 246, 412, 38, 40, 255, 158, 255, 62, 27, 84, 165, 92, 74, 272, 87, 12, 227, 116, 232, 56, 160, 51, 120, 185, 170, 185, 231, 165, 62, 170, 81, 281, 22, 126, 21, 228, 19, 57, 182, 173, 107, 367, 73, 16, 246, 42, 212, 100, 325, 43, 153, 139, 158, 191, 165, 482, 203, 212, 169, 98, 128, 13, 140, 476, 231, 23, 459, 166, 60, 73, 40, 140, 80, 41, 173, 409, 443, 19, 482, 181, 192, 427, 9, 225, 133, 177, 140, 100, 325, 16, 199, 165, 196, 263, 322, 26, 393, 111, 298, 102, 250, 201, 42, 238, 40, 155, 26, 330, 104, 433, 32, 100, 39, 76, 444, 219, 22, 97, 106, 65, 167, 30, 14, 346, 154, 189, 60, 112, 455, 171, 330, 238, 320, 26, 212, 28, 63, 306]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Create a dictionary with the tokenized sequences\n",
        "data_dict = {\n",
        "    \"input_ids\": tokenized_sequences\n",
        "}\n",
        "\n",
        "# Convert it to a Dataset object\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "split_dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "# Now we have `train_dataset` and `test_dataset`\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n"
      ],
      "metadata": {
        "id": "9U7QiKu0DE1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sentencepiece as spm\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertConfig, BertForMaskedLM, Trainer, TrainingArguments\n",
        "\n",
        "# Load the SentencePiece model\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load(\"dna_boca_tokenizer.model\")\n",
        "\n",
        "# Define a simple tokenizer wrapper to use with the transformers library\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self, sp):\n",
        "        self.sp = sp\n",
        "        self.unk_token = \"[UNK]\"\n",
        "        self.pad_token = \"[PAD]\"\n",
        "        self.cls_token = \"[CLS]\"\n",
        "        self.sep_token = \"[SEP]\"\n",
        "        self.mask_token = \"[MASK]\"\n",
        "\n",
        "    def encode(self, text):\n",
        "        input_ids = self.sp.encode(text)\n",
        "        return input_ids\n",
        "\n",
        "    def batch_encode_plus(self, texts, padding=True, truncation=True, max_length=512):\n",
        "        encoded_inputs = [self.encode(text) for text in texts]\n",
        "        if padding:\n",
        "            max_len = max(len(seq) for seq in encoded_inputs)\n",
        "            padded_inputs = [seq + [self.pad_token_id] * (max_len - len(seq)) for seq in encoded_inputs]\n",
        "            attention_masks = [[1] * len(seq) + [0] * (max_len - len(seq)) for seq in encoded_inputs]\n",
        "        else:\n",
        "            padded_inputs = encoded_inputs\n",
        "            attention_masks = [[1] * len(seq) for seq in encoded_inputs]\n",
        "        return {\"input_ids\": padded_inputs, \"attention_mask\": attention_masks}\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return self.sp.get_piece_size()\n",
        "\n",
        "    def piece_to_id(self, piece):\n",
        "        return self.sp.piece_to_id(piece)\n",
        "\n",
        "    def id_to_piece(self, id_):\n",
        "        return self.sp.id_to_piece(id_)\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self):\n",
        "        return self.piece_to_id(self.mask_token)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self):\n",
        "        return self.piece_to_id(self.pad_token)\n",
        "\n",
        "    @property\n",
        "    def cls_token_id(self):\n",
        "        return self.piece_to_id(self.cls_token)\n",
        "\n",
        "    @property\n",
        "    def sep_token_id(self):\n",
        "        return self.piece_to_id(self.sep_token)\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self):\n",
        "        return self.piece_to_id(self.unk_token)\n",
        "\n",
        "# Create an instance of the simple tokenizer\n",
        "tokenizer = SimpleTokenizer(sp)\n",
        "\n",
        "# Define a custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoded = self.tokenizer.batch_encode_plus([self.texts[idx]], max_length=self.max_length)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(encoded[\"input_ids\"][0], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(encoded[\"attention_mask\"][0], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Configure the BERT model for Masked Language Modeling (MLM)\n",
        "config = BertConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_position_embeddings=512,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    hidden_size=512,\n",
        "    intermediate_size=2048\n",
        ")\n",
        "\n",
        "# Initialize the BERT model for masked language modeling\n",
        "model = BertForMaskedLM(config)\n",
        "\n",
        "# Custom Data Collator\n",
        "class CustomDataCollatorForLanguageModeling:\n",
        "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mlm = mlm\n",
        "        self.mlm_probability = mlm_probability\n",
        "\n",
        "    def __call__(self, features):\n",
        "        input_ids = torch.stack([f[\"input_ids\"] for f in features])\n",
        "        attention_mask = torch.stack([f[\"attention_mask\"] for f in features])\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "\n",
        "        # Create special_tokens_mask where all tokens are allowed to be masked (value=0)\n",
        "        special_tokens_mask = torch.zeros_like(input_ids, dtype=torch.bool)\n",
        "\n",
        "        # Masking is only applied to the actual tokens, not the special tokens like [PAD]\n",
        "        special_tokens_mask |= (input_ids == self.tokenizer.pad_token_id)\n",
        "        special_tokens_mask |= (input_ids == self.tokenizer.cls_token_id)\n",
        "        special_tokens_mask |= (input_ids == self.tokenizer.sep_token_id)\n",
        "\n",
        "        # Apply mask to the probability matrix\n",
        "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # Only compute loss on masked tokens\n",
        "\n",
        "        input_ids[masked_indices] = self.tokenizer.mask_token_id\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "\n",
        "# Use the custom data collator\n",
        "data_collator = CustomDataCollatorForLanguageModeling(tokenizer=tokenizer)\n",
        "\n",
        "# Dummy datasets (replace with your actual datasets)\n",
        "train_texts = [\"ATGCTAGCTAGGCTA\", \"GATCGATCGATCGAT\"]\n",
        "test_texts = [\"TGCATGCATGCATGA\", \"CGATCGATCGATCGT\"]\n",
        "\n",
        "# Create the custom datasets\n",
        "train_dataset = CustomDataset(train_texts, tokenizer)\n",
        "test_dataset = CustomDataset(test_texts, tokenizer)\n",
        "\n",
        "# Set up the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./fine_tuned_dna_language_model_boca\")\n",
        "print(\"Model trained and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "Zdv8Y5juUwlW",
        "outputId": "4a12316a-c49b-4868-81f4-212caddce279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y3Pd3VudOnCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/fine_tuned_dna_language_model_boca\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgQagchYshsH",
        "outputId": "2e8264e1-282e-4701-fa00-5c186f9b729a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json  generation_config.json  model.safetensors\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM, BertTokenizer, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: tensor[idx] for key, tensor in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "# Load the tokenizer and model from the same source\n",
        "tokenizer = BertTokenizer.from_pretrained(\"/content/fine_tuned_dna_language_model_boca/\")\n",
        "model = BertForMaskedLM.from_pretrained(\"/content/advanced_boca_tokenizer.model\")\n",
        "\n",
        "# Ensure vocab size alignment by adjusting the model's embedding layer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Prepare the test dataset\n",
        "test_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I love machine learning and natural language processing.\",\n",
        "    \"Artificial intelligence is transforming the world.\"\n",
        "]\n",
        "\n",
        "# Tokenize the test texts\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Validate input IDs\n",
        "for idx, input_ids in enumerate(test_encodings[\"input_ids\"]):\n",
        "    if input_ids.max() >= tokenizer.vocab_size:\n",
        "        print(f\"Invalid token ID found in input_ids at index {idx}\")\n",
        "        print(input_ids)\n",
        "\n",
        "# Clamp input_ids to valid range if necessary\n",
        "test_encodings[\"input_ids\"] = torch.clamp(test_encodings[\"input_ids\"], max=tokenizer.vocab_size - 1)\n",
        "\n",
        "# Create a Dataset instance\n",
        "test_dataset = TextDataset(test_encodings)\n",
        "\n",
        "# Define the Trainer with evaluation arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Run the evaluation\n",
        "print(\"Starting evaluation...\")\n",
        "eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"Evaluation results: {eval_result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "csOtxFdWuATP",
        "outputId": "3330d064-35d3-4de3-8735-2b70b6a19c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for '/content/fine_tuned_dna_language_model_boca/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/fine_tuned_dna_language_model_boca/' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3db95c334af8>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load the tokenizer and model from the same source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/fine_tuned_dna_language_model_boca/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/advanced_boca_tokenizer.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2148\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/content/fine_tuned_dna_language_model_boca/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/fine_tuned_dna_language_model_boca/' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"Model config vocab size: {model.config.vocab_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNjL8ZP_xMi8",
        "outputId": "34962b2d-271f-48af-ab49-f170b443d04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer vocab size: 30522\n",
            "Model config vocab size: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, input_ids in enumerate(test_encodings[\"input_ids\"]):\n",
        "    if input_ids.max() >= tokenizer.vocab_size:\n",
        "        print(f\"Invalid token ID found in input_ids at index {idx}\")\n",
        "        print(input_ids)\n"
      ],
      "metadata": {
        "id": "lkOh0ecNxRkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}